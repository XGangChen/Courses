
Markov Property: A property where the future state depends only on the current state, not on prior states.

Markov Chain: A sequence of events in which the probability of each event depends only on the previous one.

Hidden Markov Model (HMM): A statistical model where states are hidden, and the goal is to infer these hidden states based on observable outcomes.

Markov Decision Process (MDP): A mathematical framework for modeling decision-making, involving states, actions, and rewards.

Exploration: The strategy of trying new actions to gather more information about the environment.

Exploitation: The strategy of using known information to make decisions expected to yield the highest reward.

Multi-Armed Bandit (MAB): A problem that models the trade-off between exploration and exploitation, involving a set of actions with uncertain rewards.

ε-greedy Algorithm: An algorithm that chooses the best-known action most of the time (exploitation) but occasionally tries random actions (exploration).

Upper Confidence Bounds (UCB): An algorithm that balances exploration and exploitation by considering both the estimated reward and the uncertainty in reward.

Thompson Sampling: A probabilistic approach to solving MAB problems by sampling from a distribution to estimate each action's optimality.

Reward Function: A function that assigns values to actions or states, guiding an agent's decision-making.

Beta Distribution: A probability distribution used in Thompson Sampling to model the probability of success for each action.

Action Value (Q-Value): The expected reward associated with a specific action in a specific state.

Regret: The difference in reward between the optimal action and the action taken, used to measure the loss of suboptimal choices.

Cumulative Reward: The total reward accumulated over time by an agent, a key objective in MAB problems.

Epsilon Decay: The process of gradually reducing the ε parameter in ε-greedy methods, leading to more exploitation over time.

Bound: A measure used in UCB algorithms to represent the confidence level in the value estimation of an action.
